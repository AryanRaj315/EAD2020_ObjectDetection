{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QHnVupBBn9eR"
   },
   "source": [
    "<img src=\"https://dl.fbaipublicfiles.com/detectron2/Detectron2-Logo-Horz.png\" width=\"1500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vM54r6jlKTII"
   },
   "source": [
    "# Installation detectron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 632
    },
    "colab_type": "code",
    "id": "9_FzH13EjseR",
    "outputId": "9d50107f-89ec-486d-e812-1a5e0e88fcbd"
   },
   "outputs": [],
   "source": [
    "# install dependencies:\n",
    "# (use +cu100 because colab is on CUDA 10.0)\n",
    "# ----------------------UNCOMMENT FROM HERE----------------------------------\n",
    "# !pip install -U torch==1.4+cu100 torchvision==0.5+cu100 -f https://download.pytorch.org/whl/torch_stable.html \n",
    "# !pip install cython pyyaml==5.1\n",
    "# !pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
    "# import torch, torchvision\n",
    "# torch.__version__\n",
    "# !gcc --version\n",
    "# ---------------------UNCOMMENT TILL HERE-----------------------------------\n",
    "# opencv is pre-installed on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT WHEN RUNNING THIS NOTEBOOK FOR THE FIRST TIME\n",
    "# !git clone https://github.com/facebookresearch/detectron2 detectron2_repo\n",
    "\n",
    "## You can replace cu101 with \"cu{100,92}\" or \"cpu\".\n",
    "# pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/index.html\n",
    "# !cp detectron2_repo/configs ./ -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart runtime in case you are installing detectron for the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "# common libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "try:\n",
    "    from google.colab.patches import cv2_imshow\n",
    "except:\n",
    "    os.system(f\"\"\"pip install google.colab\"\"\")\n",
    "    from google.colab.patches import cv2_imshow\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "# import some common .detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.structures import BoxMode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b2bjrfb2LDeo"
   },
   "source": [
    "# Train on a custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tjbUIhSxUdm_"
   },
   "source": [
    "If you want to use a custom dataset while also reusing detectron2’s data loaders, you will need to\n",
    "\n",
    "* Register your dataset (i.e., tell detectron2 how to obtain your dataset).\n",
    "* Optionally, register metadata for your dataset.\n",
    "\n",
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tVJoOm6LVJwW"
   },
   "source": [
    "To register your dataset to detectron2, following the [detectron2 custom dataset tutorial](https://detectron2.readthedocs.io/tutorials/datasets.html).\n",
    "Here, the dataset is in its custom format, therefore we write a function to parse it and prepare it into detectron2's standard format. See the tutorial for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folder structure to follow to directly use this notebook\n",
    "\n",
    "```\n",
    "EndoCV_Det\n",
    "│   class_list_bbox.txt  \n",
    "│\n",
    "└─── TRAIN\n",
    "│      └ images\n",
    "│      └ bbox\n",
    "└─── VAL\n",
    "│      └ images\n",
    "│      └ bbox\n",
    "└─── HOLDOUT  (Holdout set is optional)\n",
    "       └ images\n",
    "       └ bbox\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ohbyys0ig2vs"
   },
   "outputs": [],
   "source": [
    "def get_bb(img_path, img_name):\n",
    "    \"\"\"\n",
    "        This function returns annotations in the format\n",
    "        xyxy_ABS: xmin, ymin, xmax, ymax - ABSOLUTE\n",
    "        \n",
    "        Function built taking into consideration the dataset\n",
    "        provided by EAD2020\n",
    "        \n",
    "        change the path to the bounding box folder(if not training on EAD)\n",
    "        and change the loading method(if using .mat) accordingly\n",
    "    \"\"\"\n",
    "    # change the path to the bounding box folder(if not training on EAD)\n",
    "    # and change the loading method accordingly\n",
    "    bb_path = img_path[:-7]+'bbox/'\n",
    "    img = plt.imread(img_path+img_name)\n",
    "    m, n, _ = img.shape\n",
    "    labels = np.loadtxt(bb_path+img_name[:-4]+'.txt').reshape(-1, 5)\n",
    "    classes = np.empty(len(labels), dtype=np.int32)\n",
    "    xyxy = np.empty((len(labels), 4), dtype=np.int32)\n",
    "    for i, label in enumerate(labels):\n",
    "        cls, x, y, w, h = label\n",
    "        x1 = (x-w/2.)\n",
    "        x2 = x1 + w\n",
    "        y1 = (y-h/2.)\n",
    "        y2 = y1 + h\n",
    "        x1 = np.clip(int(x1 * n), 0, n-1) ; x2 = np.clip(int(x2*n), 0, n-1)\n",
    "        y1 = np.clip(int(y1 * m), 0, m-1) ; y2 = np.clip(int(y2*m), 0, m-1)\n",
    "        classes[i] = int(cls)\n",
    "        xyxy[i] = [x1, y1, x2, y2]\n",
    "        \n",
    "    return xyxy, classes\n",
    "\n",
    "def _get_dicts(phase):\n",
    "    if phase == 'train':\n",
    "        path = 'EndoCV_Det/TRAIN/images/'\n",
    "    elif phase == 'val':\n",
    "        path = 'EndoCV_Det/VAL/images/'\n",
    "    else:\n",
    "        raise(Exception('Provide either \"Train\" or \"Val\"'))\n",
    "    \n",
    "    def get_dicts():\n",
    "        dataset_dicts = []\n",
    "        img_list = os.listdir(path)\n",
    "        for idx, i in enumerate(img_list):\n",
    "            record = {}\n",
    "            img = plt.imread(path+i)\n",
    "            height, width, _ = img.shape        \n",
    "            record[\"file_name\"] = path+i\n",
    "            record[\"image_id\"] = idx\n",
    "            record[\"height\"] = height\n",
    "            record[\"width\"] = width\n",
    "            proposal_bb, proposal_logits = get_bb(path, i)\n",
    "            objs=[]\n",
    "            for j in range(len(proposal_bb)):\n",
    "                obj = {\n",
    "                    \"bbox\": [proposal_bb[j][0], proposal_bb[j][1], proposal_bb[j][2], proposal_bb[j][3]],\n",
    "                    \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "                    \"category_id\": proposal_logits[j],\n",
    "                    \"iscrowd\": 0\n",
    "                }\n",
    "                objs.append(obj)\n",
    "            record[\"annotations\"] = objs\n",
    "            record[\"thing_classes\"] = [\"specularity\",\"saturation\",\n",
    "                                          \"artifact\", \"blur\", \"contrast\", \"bubbles\",\n",
    "                                          \"instrument\", \"blood\"]\n",
    "            dataset_dicts.append(record)\n",
    "        return dataset_dicts\n",
    "    return get_dicts\n",
    "\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "for d in [\"train\", \"val\"]:\n",
    "    DatasetCatalog.register(\"endo1_\" + d, _get_dicts(d))\n",
    "#     if d == \"train\":\n",
    "    MetadataCatalog.get(\"endo1_\" + d).set(thing_classes=[\"specularity\",\"saturation\",\n",
    "                                      \"artifact\", \"blur\", \"contrast\", \"bubbles\",\n",
    "                                      \"instrument\", \"blood\"])\n",
    "#     elif d == \"val\":\n",
    "#     MetadataCatalog.get(\"endo1_\" + d).set(thing_classes=[\"specularity\",\"saturation\",\n",
    "#                                       \"artifact\", \"blur\", \"contrast\", \"bubbles\",\n",
    "#                                       \"instrument\", \"blood\"],\n",
    "#                                           pred_classes=[\"specularity\",\"saturation\",\n",
    "#                                       \"artifact\", \"blur\", \"contrast\", \"bubbles\",\n",
    "#                                       \"instrument\", \"blood\"])\n",
    "endo_metadata = MetadataCatalog.get(\"endo1_train\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6ljbWTX0Wi8E"
   },
   "source": [
    "## Visualisation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 785
    },
    "colab_type": "code",
    "id": "UkNbUzUOLYf0",
    "outputId": "6f7518a7-daf1-42d1-cef6-4951e3a8bcd1"
   },
   "outputs": [],
   "source": [
    "# Uncomment if you want to visualize training data\n",
    "\n",
    "# dataset_dicts = _get_dicts('train')()\n",
    "# for d in random.sample(dataset_dicts, 3):\n",
    "#     img = cv2.imread(d[\"file_name\"])\n",
    "#     visualizer = Visualizer(img[:, :, ::-1], metadata=endo_metadata, scale=0.5)\n",
    "#     vis = visualizer.draw_dataset_dict(d)\n",
    "#     cv2_imshow(vis.get_image()[:, :, ::-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wlqXIXXhW8dA"
   },
   "source": [
    "### To choose model:\n",
    "    detectron2_repo-->configs\n",
    "    Accordingly choose Detection or whatever you require\n",
    "    For eg:\n",
    "        \"detectron2_repo/configs/COCO-Detection/retinanet_R_50_FPN_3x.yaml\"\n",
    "### To get weights:\n",
    "    detectron2_repo-->detectron2-->model_zoo-->model_zoo.py\n",
    "    For eg: copy any link\n",
    "        \"COCO-Detection/faster_rcnn_R_50_C4_3x.yaml\": \"137849393/model_final_f97cb7.pkl\"\n",
    "    delete the mid part and add detectron2:// to the start\n",
    "        \"detectron2://COCO-Detection/faster_rcnn_R_50_C4_3x/137849393/model_final_f97cb7.pkl\"\n",
    "    use this format as a link to get the model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "7unkuuiqLdqd",
    "outputId": "80cad53a-b2dc-4f93-9602-60e08bb6b633"
   },
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(\"configs/COCO-Detection/retinanet_R_50_FPN_3x.yaml\")\n",
    "cfg.DATASETS.TRAIN = (\"endo1_train\",)\n",
    "cfg.DATASETS.TEST = (\"endo1_val\",)   # no metrics implemented for this dataset\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = \"detectron2://COCO-Detection/retinanet_R_50_FPN_3x/137849486/model_final_4cafe0.pkl\"  # initialize from model zoo\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128 \n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 8\n",
    "cfg.MODEL.RETINANET.NUM_CLASSES = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Automatically switch to a new tensorboard directory every time this cell is executed\n",
    "i = 1\n",
    "while os.path.exists(os.path.join(cfg.OUTPUT_DIR, f'run{i}')):\n",
    "    i += 1\n",
    "\n",
    "cfg.SOLVER.IMS_PER_BATCH = 8\n",
    "cfg.SOLVER.BASE_LR = 0.00025\n",
    "cfg.SOLVER.MAX_ITER = 300\n",
    "cfg.OUTPUT_DIR = os.path.join(cfg.OUTPUT_DIR, f'run{i}')\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Create trainer\n",
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)\n",
    "\n",
    "# WandB\n",
    "config = dict(cfg)\n",
    "del config['MODEL']\n",
    "wandb.init(\n",
    "    name=f\"Run {i}\",  # Name to display in wandb\n",
    "#     notes=f\"{i}th run.\",  # Notes, if any\n",
    "    notes=f\"1st real run.\",  # Notes, if any\n",
    "    project='endocv-det',  # Project name as on WandB website\n",
    "    sync_tensorboard=True,\n",
    "    config=config  # Save hyperparameters\n",
    ")\n",
    "# wandb.watch(trainer.model)\n",
    "\n",
    "# Launch Tensorboard\n",
    "%load_ext tensorboard\n",
    "!pkill tensorboard\n",
    "%tensorboard --logdir=output/run{i} --port=6007\n",
    "\n",
    "# Commence Training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0e4vdDIOXyxF"
   },
   "source": [
    "## Inference & evaluation using the trained model\n",
    "Now, let's run inference with the trained model on the balloon validation dataset. First, let's create a predictor using the model we just trained:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ya5nEuMELeq8"
   },
   "outputs": [],
   "source": [
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model\n",
    "cfg.DATASETS.TEST = (\"endo1_val\", )\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qWq1XHfDWiXO"
   },
   "source": [
    "Then, we randomly select several samples to visualize the prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "U5LhISJqWXgM",
    "outputId": "69a6ee76-b1e5-48aa-c2ee-ec5313b8adbd"
   },
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import ColorMode\n",
    "\n",
    "for t in ['train', 'val']:\n",
    "    dataset_dicts = _get_dicts(t)()\n",
    "\n",
    "    print(\"\\n\"*2 + t.upper() + \"\\n\"*2)\n",
    "    for d in random.sample(dataset_dicts, 10):\n",
    "        im = cv2.imread(d[\"file_name\"])\n",
    "        outputs = predictor(im)\n",
    "        v = Visualizer(im[:, :, ::-1],\n",
    "                       metadata=MetadataCatalog.get(f\"endo1_{t}\"), \n",
    "                       scale=0.8, \n",
    "    #                    instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels\n",
    "        )\n",
    "        v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "        cv2_imshow(v.get_image()[:, :, ::-1])\n",
    "        os.makedirs(os.path.join(cfg.OUTPUT_DIR, f'{t}_preds/'), exist_ok=True)\n",
    "        \n",
    "        # WandB\n",
    "        wandb.log({f\"{t}: {d['file_name']}\": [wandb.Image(v.get_image(), caption=f\"{d['file_name']}\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kblA1IyFvWbT"
   },
   "source": [
    "We can also evaluate its performance using AP metric implemented in COCO API.\n",
    "This gives an AP of ~70%. Not bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# &lt;The following still remains to be changed>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "h9tECBQCvMv3",
    "outputId": "9a76401f-6209-4b43-8066-cd37adcb75d0"
   },
   "outputs": [],
   "source": [
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "evaluator = COCOEvaluator(\"balloon_val\", cfg, False, output_dir=\"./output/\")\n",
    "val_loader = build_detection_test_loader(cfg, \"balloon_val\")\n",
    "inference_on_dataset(trainer.model, val_loader, evaluator)\n",
    "# another equivalent way is to use trainer.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oKBbjnLw5GGG"
   },
   "source": [
    "# Other types of builtin models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 610
    },
    "colab_type": "code",
    "id": "GYJrlXZC5M-J",
    "outputId": "e7dc1067-2b72-4686-8ca6-9d7182dd6dc9"
   },
   "outputs": [],
   "source": [
    "# Inference with a keypoint detection model\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set threshold for this model\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\")\n",
    "predictor = DefaultPredictor(cfg)\n",
    "outputs = predictor(im)\n",
    "v = Visualizer(im[:,:,::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "cv2_imshow(v.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 610
    },
    "colab_type": "code",
    "id": "roTj1N9F5uJ5",
    "outputId": "d333be49-e316-4db7-f19c-bb74c0e10364"
   },
   "outputs": [],
   "source": [
    "# Inference with a panoptic segmentation model\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\"))\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\")\n",
    "predictor = DefaultPredictor(cfg)\n",
    "panoptic_seg, segments_info = predictor(im)[\"panoptic_seg\"]\n",
    "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "v = v.draw_panoptic_seg_predictions(panoptic_seg.to(\"cpu\"), segments_info)\n",
    "cv2_imshow(v.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hiXadAb9Fv-L"
   },
   "source": [
    "# Run panoptic segmentation on a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "colab_type": "code",
    "id": "YU5_W8wJF02F",
    "outputId": "9187dd9b-3798-4e59-926c-ff10849920c2"
   },
   "outputs": [],
   "source": [
    "# This is the video we're going to process\n",
    "from IPython.display import YouTubeVideo, display\n",
    "video = YouTubeVideo(\"ll8TgCZ0plk\", width=500)\n",
    "display(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a65jM_VFF2Hr"
   },
   "outputs": [],
   "source": [
    "# Install dependencies, download the video, and crop 5 seconds for processing\n",
    "!pip install youtube-dl\n",
    "!pip uninstall -y opencv-python opencv-contrib-python\n",
    "!apt install python3-opencv  # the one pre-installed have some issues\n",
    "!youtube-dl https://www.youtube.com/watch?v=ll8TgCZ0plk -f 22 -o video.mp4\n",
    "!ffmpeg -i video.mp4 -t 00:00:06 -c:v copy video-clip.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cyA4VmKcF61k"
   },
   "outputs": [],
   "source": [
    "# Run frame-by-frame inference demo on this video (takes 3-4 minutes)\n",
    "# Using a model trained on COCO dataset\n",
    "!cd detectron2_repo && python demo/demo.py --config-file configs/COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml --video-input ../video-clip.mp4 --confidence-threshold 0.6 --output ../video-output.mkv \\\n",
    "  --opts MODEL.WEIGHTS detectron2://COCO-PanopticSegmentation/panoptic_fpn_R_101_3x/139514519/model_final_cafdb1.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OpLg_MAQGPUT"
   },
   "outputs": [],
   "source": [
    "# Download the results\n",
    "from google.colab import files\n",
    "files.download('video-output.mkv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Detectron2 Tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
